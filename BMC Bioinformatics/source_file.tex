\documentclass{bmcart}

% \PassOptionsToPackage{utf8}{inputenc}
% \documentclass[12pt]{article}
% \usepackage{arxiv}

%
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newtheorem{definition}{Definition}

\usepackage[]{algorithm2e}

\usepackage{booktabs}
\usepackage{rotating}

%%% for dags
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning, backgrounds}

%%% BibTex packages (url for website references)
\usepackage[english]{babel}
\usepackage[round, numbers]{natbib}


%%% Multipage tables
\usepackage{longtable}

% % line numbers
\usepackage{lineno}
\linenumbers

% %% DOuble spacing
\usepackage{setspace}
\doublespacing

%%% Scalable font required
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% \def\includegraphic{}
% \def\includegraphics{}

%%% Put your definitions there:
\startlocaldefs
\endlocaldefs

%%% Begin ...
\begin{document}
	
	%%% Start of article front matter
	\begin{frontmatter}
		
		\begin{fmbox}
			\dochead{Research}
			
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%                                          %%
			%% Enter the title of your article here     %%
			%%                                          %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\title{Consensus clustering for Bayesian mixture models}
			
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%                                          %%
			%% Enter the authors here                   %%
			%%                                          %%
			%% Specify information, if available,       %%
			%% in the form:                             %%
			%%   <key>={<id1>,<id2>}                    %%
			%%   <key>=                                 %%
			%% Comment or delete the keys which are     %%
			%% not used. Repeat \author command as much %%
			%% as required.                             %%
			%%                                          %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\author[
			addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
			corref={aff1},                       % id of corresponding address, if any
			% noteref={n1},                        % id's of article notes, if any
			email={stephen.coleman@mrc-bsu.cam.ac.uk}   % email address
			]{\inits{S.}\fnm{Stephen} \snm{Coleman}}
			\author[
			addressref={aff1,aff2},
			noteref={n1},
			email={paul.kirk@mrc-bsu.cam.ac.uk}
			]{\inits{P.D.W.}\fnm{Paul D.W.} \snm{Kirk}}
			\author[
			addressref={aff1,aff2},
			noteref={n1},
			email={cew54@cam.ac.uk}
			]{\inits{C.}\fnm{Chris} \snm{Wallace}}
			
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%                                          %%
			%% Enter the authors' addresses here        %%
			%%                                          %%
			%% Repeat \address commands as much as      %%
			%% required.                                %%
			%%                                          %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\address[id=aff1]{%                           % unique id
				\orgdiv{MRC Biostatistics Unit}             % department, if any
				\orgname{University of Cambridge},          % university, etc
				\city{Cambridge},                           % city
				\cny{UK}                                    % country
			}
			\address[id=aff2]{%
				\orgdiv{Cambridge Institute of Therapeutic Immunology \& Infectious Disease},
				\orgname{University of Cambridge},
				%\street{},
				%\postcode{}
				\city{Cambridge},
				\cny{UK}
			}
			
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%                                          %%
			%% Enter short notes here                   %%
			%%                                          %%
			%% Short notes will be after addresses      %%
			%% on first page.                           %%
			%%                                          %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\begin{artnotes}
				%%\note{Sample of title note}     % note to the article
				\note[id=n1]{Equal contributor} % note, connected to author
			\end{artnotes}
			
		\end{fmbox}% comment this for two column layout
		
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%%                                           %%
		%% The Abstract begins here                  %%
		%%                                           %%
		%% Please refer to the Instructions for      %%
		%% authors on https://www.biomedcentral.com/ %%
		%% and include the section headings          %%
		%% accordingly for your article type.        %%
		%%                                           %%
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		\begin{abstractbox}
			
			\begin{abstract} % abstract
				\parttitle{Background} %if any
				Cluster analysis is an integral part of precision medicine and systems biology, used to define groups of patients or biomolecules. Consensus clustering is an ensemble approach that is widely used in these areas, which combines the output from multiple runs of a non-deterministic clustering algorithm.  Here we consider the application of consensus clustering to a broad class of heuristic clustering algorithms that can be derived from Bayesian mixture models (and extensions thereof) by adopting an early stopping criterion when performing sampling-based inference for these models.  While the resulting approach is non-Bayesian, it inherits the usual benefits of consensus clustering, particularly in terms of computational scalability and providing assessments of clustering stability/robustness.
				\parttitle{Results} %if any
				In simulation studies, we show that our approach can successfully uncover the target clustering structure, while also exploring different plausible clusterings of the data.  We show that, when a parallel computation environment is available, our approach
				offers significant reductions in runtime compared to performing sampling-based Bayesian inference for the underlying model, while retaining many of the practical benefits of the Bayesian approach, such as exploring different numbers of clusters.
				We propose a heuristic to decide upon ensemble size and the early stopping criterion, and then apply consensus clustering to a clustering algorithm derived from a Bayesian integrative clustering method. 
				We use the resulting approach to perform an integrative analysis of three 'omics datasets for budding yeast and find clusters of co-expressed genes with shared regulatory proteins. We validate these clusters using data external to the analysis. These clusters can help assign likely function to understudied genes, for example \emph{GAS3} clusters with histones active in S-phase, suggesting a role in DNA replication.
				
				\parttitle{Conclustions}
				Our approach can be used as a wrapper for essentially any existing sampling-based Bayesian clustering implementation, and enables meaningful clustering analyses to be performed using such implementations, even when computational Bayesian inference is not feasible, e.g. due to poor scalability.
				This enables researchers to 
				straightforwardly extend the applicability of existing software to much larger datasets, including implementations of sophisticated models such as those that jointly model multiple datasets.
			\end{abstract}
			
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%                                          %%
			%% The keywords begin here                  %%
			%%                                          %%
			%% Put each keyword in separate \kwd{}.     %%
			%%                                          %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\begin{keyword}
				\kwd{cluster analysis}
				\kwd{cell cycle}
				\kwd{ensemble learning}
				\kwd{integrative clustering}
			\end{keyword}
			
			
			% MSC classifications codes, if any
			%\begin{keyword}[class=AMS]
			%\kwd[Primary ]{}
			%\kwd{}
			%\kwd[; secondary ]{}
			%\end{keyword}
			
		\end{abstractbox}
		%
		%\end{fmbox}% uncomment this for two column layout
		
	\end{frontmatter}
	
	\section*{Background}
	
	%\subsection{Why cluster?}
	From defining a taxonomy of disease to creating molecular sets, grouping items can help us to understand and make decisions using complex biological data. For example, grouping patients based upon disease characteristics and personal omics data may allow the identification of more homogeneous subgroups, enabling stratified medicine approaches. Defining and studying molecular sets can improve our understanding of biological systems as these sets are more interpretable than their constituent members \citep{hejblum2015time}, and study of their interactions and perturbations may have ramifications for diagnosis and drug targets \citep{bai2013strategic, emmert2014gene}. 
	The act of identifying such groups is referred to as {\em cluster analysis}. Many traditional methods such as $K$-means clustering \citep{lloyd1982least, forgy1965cluster} condition upon a fixed choice of $K$, the number of clusters.  These methods are often heuristic in nature, relying on rules of thumb to decide upon a final value for~$K$. For example, different choices of $K$ are compared under some metric such as silhouette \citep{RousseeuwSilhouettesgraphicalaid1987} or the within-cluster sum of squared errors (\textbf{SSE}) as a function of $K$. Moreover, $K$-means clustering can exhibit sensitivity to initialisation, necessitating multiple runs in practice \citep{arthur2006k}.	
	
	Another common problem is that traditional methods offer  no  measure of the stability or robustness of the  final  clustering. Returning  to the  stratified  medicine example  of  clustering patients, there might be individuals that do not clearly belong to any one particular cluster; however if only a point estimate is obtained, this information is not available.
	Ensemble methods address this problem, as well as reducing sensitivity to initialisation. These approaches have had great success in supervised learning, most famously in the form of Random Forest \citep{breiman2001random} and boosting \citep{friedman2002stochastic}. In clustering, consensus clustering \citep{monti2003consensus} is a popular method which has been implemented in \texttt{R} \citep{wilkerson2010consensusclusterplus} and to a variety of methods \citep{john2020m3c, gu2020cola} and been applied to problems such as cancer subtyping \citep{lehmann2011identification, verhaak2010integrated} and identifying subclones in single cell analysis \citep{kiselev2017sc3}. Consensus clustering uses $W$ runs of some base clustering algorithm (such as $K$-means). These $W$ proposed partitions are commonly compiled into a \emph{consensus matrix}, the $(i, j)^{th}$ entries of which contain the proportion of model runs for which the $i^{th}$ and $j^{th}$ individuals co-cluster (for this and other definitions see section 1 of the Supplementary Material), although this step is not fundamental to consensus clustering and there is a large body of literature aimed at interpreting a collection of partitions \citep[see, e.g.,][]{LiWeightedConsensusClustering2008a,  CarpinetoConsensusClusteringBased2012,  StrehlClusterEnsemblesKnowledge}. This consensus matrix provides an assessment of the stability of the clustering. Furthermore, ensembles can offer reductions in computational runtime because the individual members of the ensemble are often computationally inexpensive to fit (e.g, because they are fitted using only a subset of the available data) and because the learners in most ensemble methods are independent of each other and thus enable use of a parallel environment for each of the quicker model runs \citep{ghaemi2009survey}. 
	
	Traditional clustering methods usually condition upon a fixed choice of $K$, the number of clusters with the choice of $K$ being a difficult problem in itself. In consensus clustering, Monti {\em et al.} \cite{monti2003consensus} proposed methods for choosing $K$ using the consensus matrix and {\"U}nl{\"u} {\em et al.} \cite{UnluEstimatingnumberclusters2019} offer an approach to estimating $K$ given the collection of partitions, but each clustering run uses the same, fixed, number of clusters. An alternative clustering approach, mixture modelling, embeds the cluster analysis within a formal, statistical framework \citep{fraley2002model}. This means that models can be compared formally, and problems such as the choice of $K$ can be addressed as a model selection problem \citep{FraleyHowManyClusters1998}. Moreover, \emph{Bayesian mixture models} can be used to try to directly infer $K$ from the data. Such inference can be performed through use of a Dirichlet Process mixture model \citep{ferguson1973bayesian, AntoniakMixturesDirichletProcesses1974}, a mixture of finite mixture models \citep{richardson1997bayesian, miller2018mixture} or an over-fitted mixture model \citep{rousseau2011asymptotic}. These models and their extensions have a history of successful application to a diverse range of biological problems such as finding clusters of gene expression profiles \citep{medvedovic2002bayesian}, cell types in flow cytometry \citep{chan2008statistical, hejblum2019sequential} or scRNAseq experiments \citep{prabhakaran2016dirichlet}, and estimating protein localisation \citep{crook2018bayesian}. Bayesian mixture models can be extended to jointly model the clustering across multiple datasets \citep{kirk2012bayesian, gabasova2017clusternomics} (section 2 of the Supplementary Material).
	
	Markov chain Monte Carlo (MCMC) methods are the most common tool for performing computational Bayesian inference. In Bayesian clustering, they are used to draw a collection of clustering partitions from the posterior distribution. However, in practice, chains can become stuck in local posterior modes preventing convergence \citep[see, e.g., the Supplementary Materials of][]{strauss2020gpseudoclust} and/or can require prohibitively long runtimes, particularly when analysing high-dimensional datasets. Some MCMC methods make efforts to overcome the problem of exploration, often at the cost of increased computational cost per iteration \cite{robert2018accelerating}.  There are MCMC methods that use parallel chains to improve the scalability or reduce the bias of the Monte Carlo estimate. However, these methods have various limitations. For instance, divide-and-conquer strategies such as Asymptotically Exact, Embarrassingly Parallel MCMC \citep{NeiswangerAsymptoticallyExactEmbarrassingly2014} use subsamples of the dataset with each chain to improve scaling with the number of items being clustered. This assumes that each subsample is representative of the population, and is less helpful in situations where we have high-dimension but only moderate sample size, such as analysis of 'omics data. Alternative approaches, such as distributed MCMC \citep{MurrayDistributedMarkovchain} and coupling \citep{JacobUnbiasedMarkovchain2020} have to account for burn-in bias; moreover, coupling further assumes the chains meet in finite time and then stay together. In practice, a further challenge associated with these methods is that their implementation may necessitate a substantial redevelopment of existing software.
	
	Motivated by the lack of scalability of existing implementations of sampling-based Bayesian clustering (due to prohibitive computational runtimes, as well as poor exploration, as described above), here we aim to develop a general and straightforward procedure that exploits the flexibility of these methods, but extends their applicability. Specifically, we make use of existing sampling-based Bayesian clustering implementations, but only run them for a fixed (and relatively small) number of iterations, stopping before they have converged to their target stationary distribution. Doing this repeatedly, we obtain an ensemble of clustering partitions, which we use to perform consensus clustering. We propose a heuristic for deciding upon the ensemble size (the number of learners used, $W$) and the ensemble depth (the number of iterations, $D$), inspired by the use of scree plots in Principal Component Analysis \citep[\textbf{PCA};][]{wold1987principal}.  
		
	We show via simulation that our approach can successfully identify meaningful clustering structures. We then illustrate the use of our approach to extend the applicability of existing Bayesian clustering implementations, using as a case study the Multiple Dataset Interation \citep[\textbf{MDI};][]{kirk2012bayesian} model for Bayesian integrative clustering applied to real data. While the simulation results serve to validate our method, it is important to also evaluate methods on real data which may represent more challenging problems. For our real data, we use three 'omics datasets relating to the cell cycle of \emph{Saccharomyces cerevisiae} with the aim of inferring clusters of genes across datasets. As there is no ground truth available, we then validate these clusters using knowledge external to the analysis. 
	
	\section*{Material and methods}
	
	\subsection*{Consensus clustering for Bayesian mixture models} \label{sec:methodsCC}
	We apply consensus clustering to MCMC based Bayesian clustering models using the method described in algorithm \ref{algorithm:CCforBayesianMixtures}.
	
	\begin{algorithm} \label{algorithm:CCforBayesianMixtures}
		\KwData{\(X=(x_1, \ldots, x_N)\)}
		\KwIn{\\
			The number of chains to run, \(W\)\\
			The number of iterations within each chain, \(D\) \\
			A clustering method that uses MCMC methods to generate samples of clusterings of the data \emph{Cluster}$(X, d)$
		}
		\KwOut{\\A predicted clustering, $\hat{Y}$ \\ The consensus matrix $\mathbf{M}$}
		%	\KwResult{how to write algorithm with \LaTeX2e }
		\Begin{
			\tcc{initialise an empty consensus matrix}
			$\mathbf{M} \leftarrow \mathbf{0}_{N \times N}$\;
			\For{$w = 1$ \KwTo $W$}{
				\tcc{set the random seed controlling initialisation and MCMC moves}
				$set.seed(w)$\;
				\tcc{initialise a random partition on $X$ drawn from the prior distribution}
				$Y_{(0,w)} \leftarrow Initialise(X)$\;
				\For{$d=1$ \KwTo $D$}{
					\tcc{generate a markov chain for the membership vector}
					$Y_{(d,w)} \leftarrow Cluster(X, d)$\;
				}
				\tcc{create a coclustering matrix from the $D^{th}$ sample}
				$\mathbf{B}^{(w)} \leftarrow Y_{(D,w)}$\;
				$\mathbf{M} \leftarrow \mathbf{M} + \mathbf{B}^{(w)}$\;
			}
			$\mathbf{M} \leftarrow \frac{1}{W} \mathbf{M}$\;
			$\hat{Y} \leftarrow$ partition $X$ based upon $\mathbf{M}$\;
		}
		\caption{Consensus clustering for Bayesian mixture models. }
	\end{algorithm}
	Our application of consensus clustering has two main parameters at the ensemble level, the chain depth, $D$, and ensemble width, $W$. We infer a point clustering from the consensus matrix using the \texttt{maxpear} function \citep{fritsch2009improved} from the R package \texttt{mcclust} \citep{fritsch2012mcclust} which maximises the posterior expected adjusted Rand index between the true clustering and point estimate if the matrix is composed of samples drawn from the posterior distribution (section 3 of the Supplementary Material for details). There are alternative choices of methods to infer a point estimate which minimise different loss functions \citep[see, e.g.,][]{WadeBayesianClusterAnalysis2018, LourencoProbabilisticconsensusclustering2015, DahlSearchAlgorithmsLoss2021}.
	
	\subsubsection*{Determining the ensemble depth and width} \label{sec:ensembleChoice}
	As our ensemble sidesteps the problem of convergence within each chain, we need an alternative stopping rule for growing the ensemble in chain depth, $D$, and number of chains, $W$. We propose a heuristic based upon the consensus matrix to decide if a given value of $D$ and $W$ are sufficient. We suspect that increasing $W$ and $D$ might continuously improve the performance of the ensemble, but we observe in our simulations that these changes will become smaller and smaller for greater values, eventually converging for each of $W$ and $D$. We notice that this behaviour is analogous to PCA in that where for consensus clustering some improvement might always be expected for increasing chain depth or ensemble width, more variance will be captured by increasing the number of components used in PCA. However, increasing this number beyond some threshold has diminishing returns, diagnosed in PCA by a scree plot. Following from this, we recommend, for some set of ensemble parameters, $D' = \{d_1, \ldots, d_I\}$ and $W'=\{w_1, \ldots, w_J\}$, find the mean absolute difference of the consensus matrix for the $d_i^{th}$ iteration from $w_j$ chains to that for the $d_{(i-1)}^{th}$ iteration from $w_j$ chains and plot these values as a function of chain depth, and the analogue for sequential consensus matrices for increasing ensemble width and constant depth.
	
	If this heuristic is used, we believe that the consensus matrix and the resulting inference should be stable \citep[see, e.g.,][]{von2005towards, meinshausen2010stability}, providing a robust estimate of the clustering. In contrast, if there is still strong variation in the consensus matrix for varying chain length or number, then we believe that the inferred clustering is influenced significantly by the random initialisation and that the inferred partition is unlikely to be stable for similar datasets or reproducible for a random choice of seeds.
	
	\subsection*{Simulation study}
	
	We use a finite mixture with independent features as the data generating model within the simulation study. Within this model there exist ``irrelevant features'' \citep{law2003feature} that have global parameters rather than cluster specific parameters. The generating model is
	\begin{align} 
		p(X, c, \theta, \pi| K) =& \notag\\
		p(K) p(\pi|K) &p(\theta|K) \prod_{i=1}^N p (c_i | \pi, K)  \prod_{p=1}^P p(x_{ip} | c_i, \theta_{c_ip})^ {\phi_p} p(x_{ip} | \theta_p) ^ {(1 - \phi_p)}\label{eqn:simGenModel}
	\end{align}
	for data $X=(x_1, \ldots, x_N)$, cluster label or allocation variable $c=(c_1, \ldots, c_N)$, cluster weight $\pi=(\pi_1, \ldots, \pi_K)$, $K$ clusters and the relevance variable, $\phi \in \{0, 1\}$ with $\phi_p=1$ indicating that the $p^{th}$ feature is relevant to the clustering. We used a \emph{Gaussian} density, so $\theta_{kp} = (\mu_{kp}, \sigma^2_{kp})$. We defined three scenarios and simulated 100 datasets in each (figure \ref{fig:genData} and table \ref{table:scenarioTable}). Additional details of the simulation process and additional scenarios are included in section 4.1 of the Supplementary Materials.
		
	\begin{table}[ht!]
		\centering
		\caption{Parameters defining the simulation scenarios as used in generating data and labels. $\Delta \mu$ is the distance between neighbouring cluster means within a single feature.
			The number of relevant features (\(P_s\)) is $\sum_p \phi_p$, and $P_n = P - P_s$.}
		\begin{tabular}{l|ccccccc}
			\toprule
			\textbf{Scenario} & $N$ & $P_s$ & $P_n$ & $K$ & $\Delta \mu$ & $\sigma^2$ & $\pi$\\
			\midrule
			2D & 100 & 2 & 0 & 5 & 3.0 & 1 &  $(\frac{1}{5} , \frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5})$ \\
			Small N, large P & 50 & 500 & 0 & 5 & 1.0 & 1 &  $(\frac{1}{5} , \frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5})$\\
			Irrelevant features & 200 & 20 & 100 & 5 & 1.0 & 1 &  $(\frac{1}{5} , \frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5})$\\
			\hline
		\end{tabular}
		\label{table:scenarioTable}
	\end{table}%
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.75]{./Images/Simulations/Data.png}
		\caption{Example of generated datasets. Each row is an item being clustered and each column a feature of generated data. The 2D dataset (which is ordered by hierarchical clustering here) should enable proper mixing of chains in the MCMC. The small $N$, large $P$ case has clear structure (observable by eye). This is intended to highlight the problems of poor mixing due to high dimensions even when the generating labels are quite identifiable. In the irrelevant features case, the structure is clear in the relevant features (on the left-hand side of this heatmap). This setting is intended to test how sensitive each approach is to noise.}
		\label{fig:genData}
	\end{figure}
	
	\noindent In each of these scenarios we apply a variety of methods (listed below) and compare the inferred point clusterings to the generating labels using the Adjusted Rand Index \citep[\textbf{ARI},][]{hubert1985comparing}.
	\begin{itemize}
		\item \texttt{Mclust}, a maximum likelihood implementation of a finite mixture of Gaussian densities (for a range of modelled clusters, $K$),
		\item 10 chains of 1 million iterations, thinning to every thousandth sample for the overfitted Bayesian mixture of Gaussian densities, and
		\item A variety of consensus clustering ensembles defined by inputs of $W$ chains and $D$ iterations within each chain (see algorithm \ref{algorithm:CCforBayesianMixtures}) with $W \in \{1, 10, 30, 50, 100\}$ and $D \in \{1, 10, 100, 1000, 10000\}$ where the base learner is an overfitted Bayesian mixture of Gaussian densities.
	\end{itemize}
	Note that none of the applied methods include a model selection step and as such there is no modelling of the relevant variables. This and the unknown value of $K$ is what separates the models used and the generating model described in equation \ref{eqn:simGenModel}. More specifically, the likelihood of a point $X_n$ for each method is
	
	\begin{align} \label{eqn:mixtureModel}
		p(X_n | \mu, \Sigma, \pi) &= \sum_{k=1}^K \pi_k p(X_n | \mu_k, \Sigma_k),
	\end{align}
	where $p(X_n | \mu_k, \Sigma_k)$ is the probability density function of the multivariate Gaussian distribution parameterised by a mean vector, $\mu_k$, and a covariance matrix, $\Sigma_k$, and $\pi_k$ is the component weight such that $\sum_{k=1}^K\pi_k = 1$. The implementation of the Bayesian mixture model restricts $\Sigma_k$ to be a diagonal matrix while \texttt{Mclust} models a number of different covariance structures. Note that while we use the overfitted Bayesian mixture model, this is purely from convenience and we expect that a true Dirichlet Process mixture or a mixture of mixture models would display similar behaviour in an ensemble.
	
	The ARI is a measure of similarity between two partitions,$c_1, c_2$, corrected for chance, with 0 indicating $c_1$ is no more similar to $c_2$ than a random partition would be expected to be and a value of 1 showing that $c_1$ and $c_2$ perfectly align. Details of the methods in the simulation study can be found in sections 4.2, 4.3 and 4.4 of the Supplementary Material.
	
	\subsubsection*{\texttt{Mclust}}
	\texttt{Mclust} \citep{mclust2016scrucca} is a function from the R package \texttt{mclust}. It estimates Gaussian mixture models for $K$ clusters based upon the maximum likelihood estimator of the parameters. It initialises upon a hierarchical clustering of the data cut to $K$ clusters. A range of choices of $K$ and different covariance structures are compared and the ``best'' selected using the Bayesian information criterion, \citep[][]{schwarz1978estimating} (details in section 4.2 of the Supplementary Material).
	
	\subsubsection*{Bayesian inference}
		
	To assess within-chain convergence of our Bayesian inference we use the Geweke $Z$-score statistic \citep{geweke1991evaluating}. Of the chains that appear to behave properly we then asses across-chain convergence using $\hat{R}$ \citep{gelman1992inference} and the recent extension provided by \cite{vats2018revisiting}. If a chain has reached its stationary distribution the Geweke $Z$-score statistic is expected to be normally distributed. Normality is tested for using a Shapiro-Wilks test \citep{shapiro1965analysis}. If a chain fails this test (i.e., the associated $p$-value is less than 0.05), we assume that it has not achieved stationarity and it is excluded from the remainder of the analysis. The samples from the remaining chains are then pooled and a posterior similarity matrix (\textbf{PSM}) constructed. We use the \texttt{maxpear} function to infer a point clustering. For more details see section 4.3 of the Supplementary Material.
	
	\subsection*{Analysis of the cell cycle in budding yeast}
	\subsubsection*{Datasets}
	The cell cycle is crucial to biological growth, repair, reproduction, and development \citep{tyson2013cell, chen2004integrative, alberts2018molecular} and is highly conserved among eukaryotes \citep{alberts2018molecular}. . This means that understanding of the cell cycle of \emph{S. cerevisiae} can provide insight into a variety of cell cycle perturbations including those that occur in human cancer \citep{ingalls2007systems, chen2004integrative} and ageing \citep{jimenez2015live}. We aim to create clusters of genes that are co-expressed, have common regulatory proteins and share a biological function. To achieve this, we use three datasets that were generated using different 'omics technologies and target different aspects of the molecular biology underpinning the cell cycle process.
	\begin{itemize}
		\item Microarray profiles of RNA expression from \cite{granovskaia2010high}, comprising measurements of cell-cycle-regulated gene expression at 5-minute intervals for 200 minutes (up to three cell division cycles) and is referred to as the \textbf{time course} dataset. The cells are synchronised at the START checkpoint in late G1-phase using alpha factor arrest \citep{granovskaia2010high}. We include only the genes identified by \cite{granovskaia2010high} as having periodic expression profiles.
		\item Chromatin immunoprecipitation followed by microarray hybridization (\textbf{ChIP-chip}) data from \cite{harbison2004transcriptional}. This dataset discretizes $p$-values from tests of association between 117 DNA-binding transcriptional regulators and a set of yeast genes. Based upon a significance threshold these $p$-values are represented as either a 0 (no interaction) or a 1 (an interaction).
		\item Protein-protein interaction (\textbf{PPI}) data from BioGrid \citep{stark2006biogrid}. This database consists of of physical and genetic interactions between gene and gene products, with interactions either observed in high throughput experiments or computationally inferred. The dataset we used contained 603 proteins as columns. An entry of 1 in the $(i, j)^{th}$ cell indicates that the $i^{th}$ gene has a protein product that is believed to interact with the $j^{th}$ protein.
	\end{itemize}
	The datasets were reduced to the 551 genes with no missing data in the PPI and ChIP-chip data, as in \cite{kirk2012bayesian}. 
	
	\subsubsection*{Multiple dataset integration}
	We applied consensus clustering to MDI for our integrative analysis. Details of MDI are in section 2.2 of the Supplementary Material, but in short MDI jointly models the clustering in each dataset, inferring individual clusterings for each dataset. These partitions are informed by similar structure in the other datasets, with MDI learning this similarity as it models the partitions. The model does not assume global structure. This means that the similarity between datasets is not strongly assumed in our model; individual clusters or genes that align across datasets are based solely upon the evidence present in the data and not due to strong modelling assumptions. Thus, datasets that share less common information can be included without fearing that this will warp the final clusterings in some way.
	
	The datasets were modelled using a mixture of Gaussian processes in the time course dataset and Multinomial distributions in the ChIP-chip and PPI datasets.
		
	\section*{Results}
	\subsection*{Simulated data}
	We use the ARI between the generating labels and the inferred clustering of each method to be our metric of predictive performance.
	\begin{figure} %[!tpb]
		\centering
		\includegraphics[scale=0.75]{./Images/Simulations/simulation_model_prediction.png}
		\caption{Model performance in the 100 simulated datasets for each scenario, defined as the ARI between the generating labels and the inferred clustering. $CC(d, w)$ denotes consensus clustering using the clustering from the $d^{th}$ iteration from $w$ different chains. }
		\label{fig:simResults}
	\end{figure}
	In figure \ref{fig:simResults}, we see \texttt{Mclust} performs very well in the 2D and Small $N$, large $P$ scenarios, correctly identifying the true structure However, the irrelevant features scenario sees a collapse in performance,	\texttt{Mclust} is blinded by the irrelevant features and identifies a clustering of $K=1$. 
	
	The pooled samples from multiple long chains performs very well across all scenarios and appears to act as an upper bound on the more practical implementations of consensus clustering.
	
	Consensus clustering does uncover some of the generating structure in the data, even using a small number of short chains. With sufficiently large ensembles and chain depth, consensus clustering is close to the pooled Bayesian samples in predictive performance. It appears that for a constant chain depth increasing the ensemble width used follows a pattern of diminishing returns. There are strong initial gains for a greater ensemble width, but the improvement decreases for each successive chain. A similar pattern emerges in increasing chain length for a constant number of chains (figure \ref{fig:simResults}). 
	
	We see very little difference between the similarity matrix from the pooled samples and the consensus clustering (figure \ref{fig:simSmallNLargePPSMs}). Similar clusters emerge, and we see comparable confidence in the pairwise clusterings. For the PSMs from the individual chains, all entries are 0 or 1. This means only a single clustering is sampled within each chain, implying very little uncertainty in the partition. However, three different modes emerge across the chains showing that the chains are failing to explore the full support of the posterior distribution of the clustering and are each unrepresentative of the uncertainty in the final clustering. This shows that consensus clustering is exploring more possible clusterings than any individual chain and, as it explores a similar space to the pooled samples which might be considered more representative of the posterior distribution than any one chain, it suggests it better describes the true uncertainty present than any single chain. It also shows that pooling chains offers robustness to multi-modality (as expected for an ensemble) and the ARI for the pooled samples is an upper bound on the performance for the individual long chains.
		
	\begin{figure} %[!tpb]
		\centering
		\includegraphics[scale=0.25]{./Images/Simulations/small_n_large_p_base/comp_psms_cm_edited.png}
		\caption{Comparison of similarity matrices from a dataset for the Small $N$, large $P$ scenario. In each matrix, the $(i, j)^{th}$ entry is the proportion of clusterings for which the $i^{th}$ and $j^{th}$ items co-clustered for the method in question. In the first row the PSM of the pooled Bayesian samples is compared to the CM for CC(100, 50), with a common ordering of rows and columns in both heatmaps. In the following rows, 6 of the long chains that passed the tests of convergence are shown.}
		\label{fig:simSmallNLargePPSMs}
	\end{figure}
	
	\begin{figure} %[!tpb]
		\centering
		\includegraphics[scale=0.7]{./Images/Simulations/TimeComparison.png}	
		\caption{The time taken for different numbers of iterations of MCMC moves in $\log_{10}(seconds)$. The relationship between chain length, $D$, and the time taken is linear (the slope is approximately 1 on the $\log_{10}$ scale), with a change of intercept for different dimensions. The runtime of each Markov chain was recorded using the terminal command \texttt{time}, measured in milliseconds.}
		\label{fig:timeMCMC}
	\end{figure}
	
	Figure \ref{fig:timeMCMC} shows that chain length is directly proportional to the time taken for the chain to run. This means that using an ensemble of shorter chains, as in consensus clustering, can offer large reductions in the time cost of analysis when a parallel environment is available compared to standard Bayesian inference. Even on a laptop of 8 cores running an ensemble of 1,000 chains of length 1,000 will require approximately half as much time as running 10 chains of length 100,000 due to parallelisation, and the potential benefits are far greater when using a large computing cluster.
	
	Additional results for these and other simulations are in section 4.4 of the Supplementary Material. 
	
	\subsection*{Multi-omics analysis of the cell cycle in budding yeast}
	We use the stopping rule proposed in \ref{sec:ensembleChoice} to determine our ensemble depth and width. In figure \ref{fig:ensembleChoice}, we see that the change in the consensus matrices from increasing the ensemble depth and width is diminishing in keeping with results in the simulations. We see no strong improvement after $D=6,000$ and increasing the number of learners from 500 to 1,000 has small effect. We therefore use the largest ensemble available, a depth $D=10001$ and width $W=1000$, believing this ensemble is stable (additional evidence in section 5.1 of the Supplementary Material).
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.7]{./Images/Yeast/EnsembleChoicePlotAlt.png}
		\caption{The mean absolute difference between the sequential Consensus matrices. For a set of chain lengths, $D'=\{d_1, \ldots, d_I\}$ and number of chains, $W'=\{w_1, \ldots, w_J\}$, we take the mean of the absolute difference between the consensus matrix for $(d_i, w_j)$ and $(d_{i-1}, w_{j})$ (here $D'=\{101, 501, 1001, 2001, \ldots, 10001\}$ and $W'=\{100, 500, 1000\}$). 
		}
		\label{fig:ensembleChoice}
	\end{figure}
	
	We focus upon the genes that tend to have the same cluster label across multiple datasets. More formally, we analyse the clustering structure among genes for which $\hat{P}(c_{nl} = c_{nm}) > 0.5$, where $c_{nl}$ denotes the cluster label of gene $n$ in dataset $l$. In our analysis it is the signal shared across the time course and ChIP-chip datasets that is strongest, with 261 genes (nearly half of the genes present) in this pairing tending to have a common label, whereas only 56 genes have a common label across all three datasets. Thus, we focus upon this pairing of datasets in the results of the analysis performed using all three datasets. We show the gene expression and regulatory proteins of these genes separated by their cluster in figure \ref{fig:timepointChIPchipFused}. In figure \ref{fig:timepointChIPchipFused}, the clusters in the time series data have tight, unique signatures  (having different periods, amplitudes, or both) and in the ChIP-chip data clusters are defined by a small number of well-studied transcription factors (\textbf{TFs}) \citep[see table 2 of the Supplementary Material for details of these TFs, many of which are well known to regulate cell cycle expression,][]{simon2001serial}.
	
	\begin{sidewaysfigure}
		\centering
		\includegraphics[scale=0.62]{./Images/Yeast/timecourseChIPchipFused.png}
		\caption{The gene clusters which tend to have a common label across the time course and ChIP-chip datasets, shown in these datasets. We include only the clusters with more than one member and more than half the members having some interactions in the ChIP-chip data. Red lines for the most common transcription factors are included.}
		\label{fig:timepointChIPchipFused}
	\end{sidewaysfigure}
	
	As an example, we briefly analyse clusters 9 and 16 in greater depth. Cluster 9 has strong association with MBP1 and some interactions with SWI6, as can be seen in figure \ref{fig:timepointChIPchipFused}. The Mbp1-Swi6p complex, MBF, is associated with DNA replication \citep{iyer2001genomic}. The first time point, 0 minutes, in the time course data is at the START checkpoint, or the G1/S transition. The members of cluster 9 begin highly expressed at this point before quickly dropping in expression (in the first of the 3 cell cycles). This suggests that many transcripts are produced immediately in advance of S-phase, and thus are required for the first stages of DNA synthesis.
	These genes' descriptions \citep[found using \texttt{org.Sc.sgd.db},][and shown in table 3 of the Supplementary Material]{carlson2014org} support this hypothesis, as many of the members are associated with DNA replication, repair and/or recombination. Additionally, \emph{TOF1}, \emph{MRC1} and \emph{RAD53}, members of the replication checkpoint \citep{bando2009csm3, lao2018yeast} emerge in the cluster as do members of the cohesin complex. Cohesin is associated with sister chromatid cohesion which is established during the S-phase of the cell cycle \citep{toth1999yeast} and also contributes to transcription regulation, DNA repair, chromosome condensation, homolog pairing \citep{mehta2013cohesin}, fitting the theme of cluster 9.
	
	Cluster 16 appears to be a cluster of S-phase genes, consisting of \emph{GAS3}, \emph{NRM1} and \emph{PDS1} and the genes encoding the histones H1, H2A, H2B, H3 and H4. Histones are the chief protein components of chromatin \citep{fischle2003histone} and are important contributors to gene regulation \citep{bannister2011regulation}. They are known to peak in expression in S-phase \citep{granovskaia2010high}, which matches the first peak of this cluster early in the time series. Of the other members, \emph{NRM1} is a transcriptional co-repressor of MBF-regulated gene expression acting at the transition from G1 to S-phase \citep{de2006constraining, aligianni2009fission}. Pds1p binds to and inhibits the Esp1 class of sister separating proteins, preventing sister chromatids separation before M-phase \citep{ciosk1998esp1, toth1999yeast}. \emph{GAS3}, is not well studied. It interacts with \emph{SMT3} which regulates chromatid cohesion, chromosome segregation and DNA replication (among other things). Chromatid cohesion ensures the faithful segregation of chromosomes in mitosis and in both meiotic divisions \citep{cooper2009pds1p} and is instantiated in S-phase \citep{toth1999yeast}. These results, along with the very similar expression profile to the histone genes in the time course data, suggest that \emph{GAS3} may be more directly involved in DNA replication or chromatid cohesion than is currently believed.
	
	We attempt to perform a similar analysis using traditional Bayesian inference of MDI, but after 36 hours of runtime there is no consistency or convergence across chains.  We use the Geweke statistic and $\hat{R}$ to reduce to the five best behaved chains (none of which appear to be converged, see section 5.2 of the Supplementary Material for details). If we then compare the distribution of sampled values for the $\phi$ parameters for these long chains, the final ensemble used (D = 10001, W = 1000) and the pooled samples from the 5 long chains, then we see that the distribution of the pooled samples from the long chains (which might be believed to sampling different parts of the posterior distribution) is closer in appearance to the distributions sampled by the consensus clustering than to any single chain (figure \ref{fig:densityComparison}). Further disagreement between chains is shown in the Gene Ontology term over-representation analysis in section 5.3 of the Supplementary Material.
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.147]{./Images/Yeast/ComparisonDensitiesNoTitleEdited.png}
		\caption{The sampled values for the $\phi$ parameters from the long chains, their pooled samples and the consensus using 1000 chains of depth 10,001. The long chains display a variety of behaviours. Across chains there is no clear consensus on the nature of the posterior distribution. The samples from any single chain are not particularly close to the behaviour of the pooled samples across all three parameters. It is the consensus clustering that most approaches this pooled behaviour.}
		\label{fig:densityComparison}
	\end{figure}
	
	\section*{Discussion}
	Our proposed method has demonstrated good performance on simulation studies, uncovering the generating structure in many cases and performing comparably to \texttt{Mclust} and long chains in many scenarios. We saw that when the chains are sufficiently deep that the ensemble approximates Bayesian inference, as shown by the similarity between the PSMs and the CM in the 2D scenario where the individual chains do not become trapped in a single mode. However, we have shown that if a finite Markov chain fails to describe the full posterior distribution, our method frequently has better ability to represent several modes in the data than individual chains and thus offers a more consistent and reproducible analysis. We also showed that the ensemble of short chains is more robust to irrelevant features than \texttt{Mclust}. Furthermore, an ensemble of short chains is significantly faster in a parallel environment than inference using individual long chains. 
	
	We proposed a method of assessing ensemble stability and deciding upon ensemble size which we used when performing an integrative analysis of yeast cell cycle data using MDI, an extension of Bayesian mixture models that jointly models multiple datasets. 
	We uncovered many genes with shared signal across several datasets and explored the meaning of some of the inferred clusters using data external to the analysis. We found biologically meaningful results as well as signal for possibly novel biology. We also showed that individual chains for the existing implementation of MDI do not converge in a practical length of time, having run 10 chains for 36 hours with no consistent behaviour across chains. This means that Bayesian inference of the MDI model is not practical on this dataset with the software currently available.
	
	However, consensus clustering does lose the theoretical framework of true Bayesian inference. We attempt to mitigate this with our assessment of stability in the ensemble, but this diagnosis is heuristic and subjective, and while there is empirical evidence for its success, it lacks the formal results for the tests of model convergence for Bayesian inference.
	
	More generally, we have benchmarked the use of an ensemble of Bayesian mixture models, showing that this approach can infer meaningful clusterings and overcomes the problem of multi-modality in the likelihood surface even in high dimensions, thereby providing more stable clusterings than individual long chains that are prone to becoming trapped in individual modes. We also show that the ensemble can be significantly quicker to run. In our multi-omics study we have demonstrated that the method can be applied as a wrapper to more complex Bayesian clustering methods using existing implementations and that this provides meaningful results even when individual chains fail to converge. This enables greater application of complex Bayesian clustering methods without requiring re-implementation using more clever MCMC methods, a process that would involve a significant investment of human time.
	
	We expect that researchers interested in applying some of the Bayesian integrative clustering models such as MDI and Clusternomics \citep{gabasova2017clusternomics} will be enabled to do so, as consensus clustering overcomes some of the unwieldiness of existing implementations of these complex models. More generally, we expect that our method will be useful to researchers performing cluster analysis of high-dimensional data where the runtime of MCMC methods becomes too onerous and multi-modality is more likely to be present.
	
	\begin{backmatter}
		
		\section*{Abbreviations}%% if any
		ARI: Adjusted Rand Index \newline 
		ChIP-chip: Chromatin immunoprecipitation followed by microarray hybridization \newline
		CM: Consensus Matrix \newline
		MCMC: Markov chain Monte Carlo \newline
		MDI: Multiple Dataset Integration \newline
		PCA: Principal Component Analysis \newline
		PPI: Protein-Protein Interaction \newline
		PSM: Posterior Similarity Matrix \newline
		SSE: Sum of Squared Errors \newline
		TF: Transcription Factor
		
		\section*{Declarations}
		Not applicable.
		
		\section*{Ethics approval and consent to participate}%% if any
		Not applicable.
		
		\section*{Consent for publication}
		Not applicable.
		
		\section*{Availability of data and materials}%% if any
		The code and datasets supporting the conclusions of this article are available in the github repository, \texttt{https://github.com/stcolema/ConsensusClusteringForBayesianMixtureModels}.
		
		\section*{Competing interests}
		The authors declare that they have no competing interests.
		
		\section*{Funding}%% if any
		This work was funded by the MRC (MC UU 00002/4, MC UU 00002/13) and supported by the NIHR Cambridge Biomedical Research Centre (BRC-1215-20014). The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care. This research was funded in whole, or in part, by the Wellcome Trust [WT107881]. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.
		
		%	\section*{Consent for publication}%% if any
		%	Text for this section\ldots
		
		\section*{Authors' contributions}
		SC designed the simulation study with contributions from PK and CW, performed the analyses and wrote the manuscript. PK and CW provided an equal contribution of joint supervision, directing the research and provided suggestions such as the stopping rule. All contributed to interpreting the results of the analyses. All authors revised and approved the final manuscript.
		
		\section*{Acknowledgements}%% if any
		Not applicable.
		
		\vspace*{-12pt}
		
		\bibliographystyle{vancouver}
		\bibliography{CCbib}  
		
		\section*{Additional Files}
		\subsection*{Additional file 1 --- Supplementary materials}
		Additional relevant theory, background and results. This includes some more formal definitions, details of Bayesian mixture models and MDI, the general consensus clustering algorithm, additional simulations and the generating algorithm used, steps in assessing Bayesian model convergence in both the simulated datasets and yeast analysis, a table of the transcription factors that define the clustering in the ChIP-chip dataset, a table of the gene descriptions for some of the clusters that emerge across the time course and ChIP-chip datasets and Gene Ontology term over-representation analysis of the clusterings from the yeast datasets. (PDF, 10MB)
		
	\end{backmatter}
\end{document}
